#!/usr/bin/env python3
"""
Benchmark Script for Mental AI Assistant
Measures STT latency, LLM tokens/sec, and end-to-end turn time
"""

import time
import json
import requests
import statistics
from pathlib import Path
from typing import List, Dict, Tuple
import sys

# Service URLs
MEDIA_SERVICE = "http://localhost:8001"
SPEECH_SERVICE = "http://localhost:8002"
REASONING_SERVICE = "http://localhost:8003"

# Test utterances (Tamil phrases)
TEST_UTTERANCES = [
    "à®¨à®¾à®©à¯ à®®à®¿à®•à®µà¯à®®à¯ à®•à®µà®²à¯ˆà®¯à®¾à®• à®‰à®£à®°à¯à®•à®¿à®±à¯‡à®©à¯",  # I feel very anxious
    "à®à®©à¯à®©à®¾à®²à¯ à®¤à¯‚à®™à¯à®• à®®à¯à®Ÿà®¿à®¯à®µà®¿à®²à¯à®²à¯ˆ",  # I can't sleep
    "à®à®©à®•à¯à®•à¯ à®¯à®¾à®°à®¿à®Ÿà®®à¯à®®à¯ à®ªà¯‡à®š à®®à¯à®Ÿà®¿à®¯à®µà®¿à®²à¯à®²à¯ˆ",  # I can't talk to anyone
    "à®¨à®¾à®©à¯ à®¤à®©à®¿à®®à¯ˆà®¯à®¾à®• à®‰à®£à®°à¯à®•à®¿à®±à¯‡à®©à¯",  # I feel lonely
    "à®à®©à®•à¯à®•à¯ à®‰à®¤à®µà®¿ à®¤à¯‡à®µà¯ˆ",  # I need help
    "à®à®©à¯ à®µà®¾à®´à¯à®•à¯à®•à¯ˆ à®…à®°à¯à®¤à¯à®¤à®®à®±à¯à®±à®¤à®¾à®• à®‰à®£à®°à¯à®•à®¿à®±à®¤à¯",  # My life feels meaningless
    "à®¨à®¾à®©à¯ à®®à®¿à®•à®µà¯à®®à¯ à®šà¯‹à®°à¯à®µà®¾à®• à®‡à®°à¯à®•à¯à®•à®¿à®±à¯‡à®©à¯",  # I feel very tired
    "à®à®©à®•à¯à®•à¯ à®¯à®¾à®°à¯ˆà®¯à¯à®®à¯ à®¨à®®à¯à®ª à®®à¯à®Ÿà®¿à®¯à®µà®¿à®²à¯à®²à¯ˆ",  # I can't trust anyone
    "à®¨à®¾à®©à¯ à®ªà®¯à®ªà¯à®ªà®Ÿà¯à®•à®¿à®±à¯‡à®©à¯",  # I'm scared
    "à®à®©à®•à¯à®•à¯ à®†à®¤à®°à®µà¯ à®¤à¯‡à®µà¯ˆ",  # I need support
]

# Additional test phrases for broader coverage
EXTENDED_UTTERANCES = [
    "à®¨à®¾à®©à¯ à®®à®•à®¿à®´à¯à®šà¯à®šà®¿à®¯à®¾à®• à®‡à®°à¯à®•à¯à®• à®µà®¿à®°à¯à®®à¯à®ªà¯à®•à®¿à®±à¯‡à®©à¯",  # I want to be happy
    "à®à®©à®•à¯à®•à¯ à®¨à®²à¯à®² à®¨à®£à¯à®ªà®°à¯à®•à®³à¯ à®µà¯‡à®£à¯à®Ÿà¯à®®à¯",  # I want good friends
    "à®à®©à¯ à®•à¯à®Ÿà¯à®®à¯à®ªà®®à¯ à®à®©à¯à®©à¯ˆ à®ªà¯à®°à®¿à®¨à¯à®¤à¯ à®•à¯Šà®³à¯à®³à®µà®¿à®²à¯à®²à¯ˆ",  # My family doesnà¯t understand me
    "à®¨à®¾à®©à¯ à®à®©à¯ à®à®¤à®¿à®°à¯à®•à®¾à®²à®¤à¯à®¤à¯ˆà®ªà¯ à®ªà®±à¯à®±à®¿ à®•à®µà®²à¯ˆà®ªà¯à®ªà®Ÿà¯à®•à®¿à®±à¯‡à®©à¯",  # I worry about my future
    "à®à®©à®•à¯à®•à¯ à®¯à®¾à®°à®¾à®µà®¤à¯ à®•à¯‡à®Ÿà¯à®• à®µà¯‡à®£à¯à®Ÿà¯à®®à¯",  # I need someone to listen
    "à®¨à®¾à®©à¯ à®®à®© à®…à®´à¯à®¤à¯à®¤à®¤à¯à®¤à®¿à®²à¯ à®‡à®°à¯à®•à¯à®•à®¿à®±à¯‡à®©à¯",  # I am stressed
    "à®à®©à®•à¯à®•à¯ à®“à®¯à¯à®µà¯ à®¤à¯‡à®µà¯ˆ",  # I need rest
    "à®¨à®¾à®©à¯ à®šà¯‹à®•à®®à®¾à®• à®‰à®£à®°à¯à®•à®¿à®±à¯‡à®©à¯",  # I feel sad
    "à®à®©à®•à¯à®•à¯ à®¨à®®à¯à®ªà®¿à®•à¯à®•à¯ˆ à®µà¯‡à®£à¯à®Ÿà¯à®®à¯",  # I need hope
    "à®¨à®¾à®©à¯ à®•à¯à®´à®ªà¯à®ªà®®à®¾à®• à®‡à®°à¯à®•à¯à®•à®¿à®±à¯‡à®©à¯",  # I am confused
]

ALL_UTTERANCES = TEST_UTTERANCES + EXTENDED_UTTERANCES


class BenchmarkResults:
    """Store and analyze benchmark results"""
    
    def __init__(self):
        self.stt_latencies: List[float] = []
        self.reasoning_latencies: List[float] = []
        self.tts_latencies: List[float] = []
        self.e2e_latencies: List[float] = []
        self.stt_tokens_per_sec: List[float] = []
        self.reasoning_tokens_per_sec: List[float] = []
        
    def add_result(self, stt_ms: float, reasoning_ms: float, tts_ms: float, 
                   e2e_ms: float, transcript_len: int, response_len: int):
        self.stt_latencies.append(stt_ms)
        self.reasoning_latencies.append(reasoning_ms)
        self.tts_latencies.append(tts_ms)
        self.e2e_latencies.append(e2e_ms)
        
        # Calculate tokens/sec (rough estimate: 1 char â‰ˆ 0.5 tokens)
        if stt_ms > 0:
            self.stt_tokens_per_sec.append((transcript_len * 0.5) / (stt_ms / 1000))
        if reasoning_ms > 0:
            self.reasoning_tokens_per_sec.append((response_len * 0.5) / (reasoning_ms / 1000))
    
    def summary(self) -> Dict:
        """Generate summary statistics"""
        def stats(data: List[float]) -> Dict:
            if not data:
                return {"mean": 0, "median": 0, "p95": 0, "p99": 0}
            return {
                "mean": statistics.mean(data),
                "median": statistics.median(data),
                "p95": sorted(data)[int(len(data) * 0.95)] if len(data) > 0 else 0,
                "p99": sorted(data)[int(len(data) * 0.99)] if len(data) > 0 else 0,
            }
        
        return {
            "stt_latency_ms": stats(self.stt_latencies),
            "reasoning_latency_ms": stats(self.reasoning_latencies),
            "tts_latency_ms": stats(self.tts_latencies),
            "e2e_latency_ms": stats(self.e2e_latencies),
            "stt_tokens_per_sec": stats(self.stt_tokens_per_sec),
            "reasoning_tokens_per_sec": stats(self.reasoning_tokens_per_sec),
        }


def check_services() -> bool:
    """Check if all services are running"""
    print("ğŸ” Checking services...")
    services = [
        ("Media", MEDIA_SERVICE),
        ("Speech", SPEECH_SERVICE),
        ("Reasoning", REASONING_SERVICE),
    ]
    
    all_healthy = True
    for name, url in services:
        try:
            resp = requests.get(f"{url}/health", timeout=5)
            if resp.ok:
                print(f"  âœ… {name} service: OK")
            else:
                print(f"  âŒ {name} service: FAILED (status {resp.status_code})")
                all_healthy = False
        except Exception as e:
            print(f"  âŒ {name} service: UNREACHABLE ({e})")
            all_healthy = False
    
    return all_healthy


def synthesize_audio(text: str) -> Tuple[bytes, float]:
    """
    Synthesize audio from text (for testing).
    In real benchmark, use pre-recorded Tamil audio files.
    """
    start = time.time()
    resp = requests.post(
        f"{SPEECH_SERVICE}/tts/speak",
        json={"text": text, "locale": "ta-IN"},
        timeout=30
    )
    latency_ms = (time.time() - start) * 1000
    
    if not resp.ok:
        raise Exception(f"TTS failed: {resp.text}")
    
    data = resp.json()
    audio_url = f"{SPEECH_SERVICE}{data.get('file_url') or data.get('audioUrl')}"
    
    # Download audio
    audio_resp = requests.get(audio_url, timeout=10)
    return audio_resp.content, latency_ms


def run_e2e_benchmark(text: str, session_id: str, seq_index: int, 
                     audio_file: Path = None) -> Dict:
    """
    Run end-to-end benchmark: audio â†’ STT â†’ reasoning â†’ TTS
    """
    print(f"\nğŸ¤ Testing: {text[:50]}...")
    
    e2e_start = time.time()
    
    # Step 1: Synthesize audio (or use pre-recorded file)
    if audio_file and audio_file.exists():
        with open(audio_file, 'rb') as f:
            audio_data = f.read()
        tts_synth_ms = 0  # Pre-recorded, no synthesis time
    else:
        print("  âš ï¸  No audio file; synthesizing from text (less accurate)")
        audio_data, tts_synth_ms = synthesize_audio(text)
    
    # Step 2: Upload audio chunk (includes STT)
    stt_start = time.time()
    files = {'file': ('audio.wav', audio_data, 'audio/wav')}
    resp = requests.post(
        f"{MEDIA_SERVICE}/media/chunk-upload",
        params={"session_id": session_id, "sequence_index": seq_index},
        files=files,
        timeout=30
    )
    stt_latency_ms = (time.time() - stt_start) * 1000
    
    if not resp.ok:
        raise Exception(f"STT failed: {resp.text}")
    
    stt_data = resp.json()
    transcript = stt_data.get("transcript", "")
    stt_timing = stt_data.get("timing_ms", stt_latency_ms)
    
    print(f"  ğŸ“ Transcript: {transcript} ({stt_timing:.0f}ms)")
    
    # Step 3: Generate response
    reasoning_start = time.time()
    resp = requests.post(
        f"{REASONING_SERVICE}/respond",
        json={
            "session_id": session_id,
            "user_input": transcript,
            "locale": "ta-IN"
        },
        timeout=30
    )
    reasoning_latency_ms = (time.time() - reasoning_start) * 1000
    
    if not resp.ok:
        raise Exception(f"Reasoning failed: {resp.text}")
    
    reasoning_data = resp.json()
    response_text = reasoning_data.get("response", "")
    reasoning_timing = reasoning_data.get("timing_ms", reasoning_latency_ms)
    
    print(f"  ğŸ’­ Response: {response_text[:60]}... ({reasoning_timing:.0f}ms)")
    
    # Step 4: TTS for response
    tts_start = time.time()
    resp = requests.post(
        f"{SPEECH_SERVICE}/tts/speak",
        json={"text": response_text, "locale": "ta-IN"},
        timeout=30
    )
    tts_latency_ms = (time.time() - tts_start) * 1000
    
    if not resp.ok:
        raise Exception(f"TTS failed: {resp.text}")
    
    tts_data = resp.json()
    tts_timing = tts_data.get("timing_ms", tts_latency_ms)
    
    print(f"  ğŸ”Š Audio ready ({tts_timing:.0f}ms)")
    
    e2e_latency_ms = (time.time() - e2e_start) * 1000
    print(f"  â±ï¸  E2E: {e2e_latency_ms:.0f}ms")
    
    return {
        "transcript": transcript,
        "response": response_text,
        "stt_ms": stt_timing,
        "reasoning_ms": reasoning_timing,
        "tts_ms": tts_timing,
        "e2e_ms": e2e_latency_ms,
    }


def main():
    print("=" * 60)
    print("ğŸš€ Mental AI Assistant - Benchmark Suite")
    print("=" * 60)
    
    # Check services
    if not check_services():
        print("\nâŒ Some services are not running. Start them first:")
        print("   ./start-services.sh")
        sys.exit(1)
    
    print("\nğŸ“Š Running benchmark with 20 Tamil utterances...")
    print("=" * 60)
    
    results = BenchmarkResults()
    session_id = f"bench_{int(time.time())}"
    
    # Run benchmarks
    test_set = ALL_UTTERANCES[:20]  # Use first 20
    for i, text in enumerate(test_set):
        try:
            result = run_e2e_benchmark(text, session_id, i)
            results.add_result(
                stt_ms=result["stt_ms"],
                reasoning_ms=result["reasoning_ms"],
                tts_ms=result["tts_ms"],
                e2e_ms=result["e2e_ms"],
                transcript_len=len(result["transcript"]),
                response_len=len(result["response"]),
            )
        except Exception as e:
            print(f"  âŒ Error: {e}")
            continue
    
    # Print summary
    print("\n" + "=" * 60)
    print("ğŸ“ˆ BENCHMARK RESULTS")
    print("=" * 60)
    
    summary = results.summary()
    
    print("\nğŸ¤ STT Latency:")
    print(f"  Mean:   {summary['stt_latency_ms']['mean']:.0f}ms")
    print(f"  Median: {summary['stt_latency_ms']['median']:.0f}ms")
    print(f"  P95:    {summary['stt_latency_ms']['p95']:.0f}ms")
    print(f"  P99:    {summary['stt_latency_ms']['p99']:.0f}ms")
    
    print("\nğŸ’­ Reasoning Latency:")
    print(f"  Mean:   {summary['reasoning_latency_ms']['mean']:.0f}ms")
    print(f"  Median: {summary['reasoning_latency_ms']['median']:.0f}ms")
    print(f"  P95:    {summary['reasoning_latency_ms']['p95']:.0f}ms")
    print(f"  P99:    {summary['reasoning_latency_ms']['p99']:.0f}ms")
    
    print("\nğŸ”Š TTS Latency:")
    print(f"  Mean:   {summary['tts_latency_ms']['mean']:.0f}ms")
    print(f"  Median: {summary['tts_latency_ms']['median']:.0f}ms")
    print(f"  P95:    {summary['tts_latency_ms']['p95']:.0f}ms")
    print(f"  P99:    {summary['tts_latency_ms']['p99']:.0f}ms")
    
    print("\nâ±ï¸  End-to-End Turn Time:")
    print(f"  Mean:   {summary['e2e_latency_ms']['mean']:.0f}ms ({summary['e2e_latency_ms']['mean']/1000:.1f}s)")
    print(f"  Median: {summary['e2e_latency_ms']['median']:.0f}ms ({summary['e2e_latency_ms']['median']/1000:.1f}s)")
    print(f"  P95:    {summary['e2e_latency_ms']['p95']:.0f}ms ({summary['e2e_latency_ms']['p95']/1000:.1f}s)")
    print(f"  P99:    {summary['e2e_latency_ms']['p99']:.0f}ms ({summary['e2e_latency_ms']['p99']/1000:.1f}s)")
    
    print("\nğŸ¯ Throughput:")
    print(f"  STT:       {summary['stt_tokens_per_sec']['mean']:.1f} tokens/sec")
    print(f"  Reasoning: {summary['reasoning_tokens_per_sec']['mean']:.1f} tokens/sec")
    
    # Save results
    output_file = Path("benchmark-results.json")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            "timestamp": time.time(),
            "session_id": session_id,
            "summary": summary,
            "raw_results": {
                "stt_latencies": results.stt_latencies,
                "reasoning_latencies": results.reasoning_latencies,
                "tts_latencies": results.tts_latencies,
                "e2e_latencies": results.e2e_latencies,
            }
        }, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ Results saved to: {output_file}")
    
    # Acceptance check
    print("\n" + "=" * 60)
    print("âœ… ACCEPTANCE CHECK")
    print("=" * 60)
    
    mean_e2e = summary['e2e_latency_ms']['mean'] / 1000
    target = 10.0  # 10 seconds
    
    if mean_e2e <= target:
        print(f"âœ… PASS: Mean E2E ({mean_e2e:.1f}s) â‰¤ target ({target}s)")
    else:
        print(f"âš ï¸  WARN: Mean E2E ({mean_e2e:.1f}s) > target ({target}s)")
        print("   Consider: GPU acceleration, smaller models, or caching")
    
    print("\nâœ¨ Benchmark complete!")


if __name__ == "__main__":
    main()
