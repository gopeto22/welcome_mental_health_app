# Mental AI Assistant - Tamil Voice Support

**Client-first mental health support with Tamil voice interaction**

Audio stays on your device. Only text is processed when generating responses (Phase A). Phase B will move all processing on-device.

---

## ğŸ“– **NEW TO THIS PROJECT?**

ğŸ‘‰ **Start here: [DELIVERY.md](./DELIVERY.md)** - Complete delivery summary, next steps, and acceptance criteria  
ğŸ‘‰ **Or read: [TRANSFORMATION.md](./TRANSFORMATION.md)** - Full before/after transformation overview  
ğŸ‘‰ **Navigation: [DOCS.md](./DOCS.md)** - Guide to all documentation

---

## ï¿½ï¸ Architecture

```
mental-ai-assistant/
â”œâ”€â”€ frontend/           # React + Vite + TypeScript + Tailwind
â””â”€â”€ services/          # FastAPI microservices (local only)
    â”œâ”€â”€ media-service/     # Port 8001 - Audio chunk handling
    â”œâ”€â”€ speech-service/    # Port 8002 - STT/TTS
    â””â”€â”€ reasoning-service/ # Port 8003 - LLM + Safety guardrails
```

## ğŸš€ Quick Start

### Prerequisites

- **Node.js** â‰¥18
- **Python** 3.11+
- **FFmpeg** (for audio processing)

### 1. Install Dependencies

```bash
# Frontend
cd frontend
npm install

# Each service
cd services/media-service && pip install -r requirements.txt
cd ../speech-service && pip install -r requirements.txt
cd ../reasoning-service && pip install -r requirements.txt
```

### 2. Configure Environment

Copy `.env.example` to `.env` in each directory and add your API keys (Phase A only):

```bash
cp frontend/.env.example frontend/.env
cp services/speech-service/.env.example services/speech-service/.env
cp services/reasoning-service/.env.example services/reasoning-service/.env
```

### 3. Run Services

```bash
# Terminal 1 - Media Service
cd services/media-service
uvicorn app.main:app --reload --port 8001

# Terminal 2 - Speech Service
cd services/speech-service
uvicorn app.main:app --reload --port 8002

# Terminal 3 - Reasoning Service
cd services/reasoning-service
uvicorn app.main:app --reload --port 8003

# Terminal 4 - Frontend
cd frontend
npm run dev
```

Open **http://localhost:5173**

## ğŸ¯ Features

- ğŸ¤ **Push-to-talk voice recording** with device selection
- ğŸ—£ï¸ **Tamil speech recognition** (partial + final transcripts)
- ğŸ§  **Safety-checked AI responses** with crisis detection
- ğŸ”Š **Tamil text-to-speech** replies
- ğŸš¨ **Crisis help** always visible with helpline info
- ğŸ“ **Real-time transcript** display
- â™¿ **Accessible** keyboard control, screen reader support

## ğŸ“‹ Phase A vs Phase B

### Phase A (Current - Accuracy First)
- **STT**: Groq Whisper large-v3-turbo API
- **TTS**: Google Cloud TTS (ta-IN)
- **LLM**: Groq Llama-3.3-70B API
- Audio stays on device; only text sent to APIs

### Phase B (Future - Privacy First)
- **STT**: On-device Whisper Tiny/Base
- **TTS**: System TTS or bundled MMS-TTS
- **LLM**: Quantized 1-3B local model
- Everything runs on-device

## ğŸ§ª Testing

```bash
# Backend tests
cd services/speech-service
pytest

cd services/reasoning-service
pytest

# Frontend tests
cd frontend
npm test
```

## ğŸ”’ Privacy & Safety

- Audio files never leave the device
- Text-only processing in Phase A (with user consent)
- Crisis detection with immediate helpline information
- No cloud storage of conversations
- Local-only risk event logging

## ğŸ“ Crisis Resources

**Tamil Nadu State Mental Health Helpline**: 044-46464646  
**National Crisis Helpline (India)**: 9152987821

## ï¿½ License

Internal use only. Not for redistribution.

### Prerequisites
- Node.js 18+
- npm or yarn

### Installation

```bash
# Install dependencies
npm install

# Create environment file
cp .env.example .env

# Configure your backend API URL in .env
VITE_API_URL=http://localhost:8000

# Start development server
npm run dev
```

The app will be available at `http://localhost:8080`

### Build for Production
```bash
npm run build
```

## ğŸ”§ Backend Setup (Separate Deployment)

The backend consists of 4 FastAPI microservices. See `BACKEND.md` for detailed setup.

### Quick Start
```bash
# Each service should be run separately or via Docker Compose
cd services/media-service && uvicorn app.main:app --port 8003
cd services/transcription-service && uvicorn app.main:app --port 8005
cd services/reasoning-service && uvicorn app.main:app --port 8007
cd services/auth-gateway && uvicorn app.main:app --port 8001
```

### Required API Keys
- `GROQ_API_KEY` - For Whisper STT and LLaMA reasoning
- `GOOGLE_TTS_PROJECT_ID` & `GOOGLE_TTS_KEY` - For Tamil TTS
- `SUPABASE_URL` & `SUPABASE_ANON_KEY` - For authentication (stub)

## ğŸ¨ Design System

The UI uses a calming color palette optimized for mental wellness:

- **Primary**: Purple gradient (262Â° 52% 47%)
- **Secondary**: Blue tones (220Â° 60% 60%)
- **Accent**: Teal (180Â° 60% 55%)
- **Gradients**: Soft purple-blue transitions
- **Typography**: Clean, readable fonts with ample spacing

All colors are semantic tokens defined in `src/index.css` and `tailwind.config.ts`.

## ğŸ”’ Safety Features

### Pre-Check (Before LLM)
- Detects self-harm keywords
- Identifies medical advice requests
- Flags acute crisis situations

### Post-Check (After LLM)
- Validates response safety
- Replaces unsafe content with safe templates
- Triggers clinician alerts when needed

### Visual Indicators
- ğŸ”´ Safety alerts shown in transcript
- âš ï¸ Crisis warnings with support resources
- ğŸ“Š Risk flags stored per session

## ğŸ“± Usage

1. **Select Microphone**: Choose your audio input device
2. **Press & Hold**: Push the microphone button to speak
3. **Speak in Tamil**: Your speech is transcribed in real-time
4. **Get Response**: AI processes your message with safety checks
5. **Listen**: Tamil audio response plays automatically

## ğŸ§ª Development

### Project Structure
```
src/
â”œâ”€ components/
â”‚  â”œâ”€ DevicePicker.tsx       # Audio device selection
â”‚  â”œâ”€ VoiceButton.tsx        # Push-to-talk control
â”‚  â””â”€ TranscriptPane.tsx     # Message history
â”œâ”€ hooks/
â”‚  â””â”€ useRecorder.ts         # MediaRecorder with chunking
â”œâ”€ state/
â”‚  â””â”€ useSessionStore.ts     # Zustand session state
â”œâ”€ api/
â”‚  â””â”€ client.ts              # Axios API client
â””â”€ pages/
   â””â”€ Demo.tsx               # Main application page
```

### API Integration

The frontend expects these backend endpoints:

```typescript
POST /media/chunk-upload        // Upload audio chunks
POST /transcribe/chunk          // Get transcript from chunk
POST /respond                   // Get AI response with safety check
POST /tts/generate              // Generate Tamil audio
GET  /health                    // Service health checks
```

## ğŸŒ Environment Variables

### Frontend (.env)
```bash
VITE_API_URL=http://localhost:8000
```

### Backend Services
See individual service README files for complete environment configuration.

## ğŸ“Š Current Status

**Week 1 MVP - âœ… Complete**
- âœ… Frontend UI with push-to-talk
- âœ… Device picker and audio recording
- âœ… Transcript display with safety indicators
- âœ… API client ready for backend integration
- â³ Backend services (deploy separately)

**Non-Goals (Week 1)**
- âŒ Fine-tuning models
- âŒ Analytics dashboards  
- âŒ Session persistence beyond memory
- âŒ User authentication (stub only)

## ğŸ”— Related Documentation

- `BACKEND.md` - Backend microservices setup
- `.env.example` - Environment configuration template
- Service READMEs in `services/*/README.md`

## ğŸ“„ License

MIT

## ğŸ™ Acknowledgments

Inspired by the TalentSync interview agent architecture and mental wellness design patterns from Calm and Headspace.
